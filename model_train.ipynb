{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from cnnf.model_cifar import WideResNet\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from train import train_adv, test, test_pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#args\n",
    "class Arg:\n",
    "    pass\n",
    "#opt params\n",
    "args = Arg()\n",
    "args.batch_size = 256\n",
    "args.test_batch_size = 128\n",
    "args.epochs = 500\n",
    "args.lr = 0.05\n",
    "args.power = 0.9\n",
    "args.momentum = 0.9\n",
    "args.wd = 5e-4\n",
    "args.grad_clip = True\n",
    "args.dataset = 'cifar10'\n",
    "args.schedule = 'poly'\n",
    "args.no_cuda = True\n",
    "args.seed = 1\n",
    "args.log_interval = 400\n",
    "\n",
    "#adver training params\n",
    "args.eps = 0.063\n",
    "args.eps_iter = 0.02\n",
    "args.nb_iter = 7\n",
    "args.clean = 'supclean'\n",
    "\n",
    "#hyper params\n",
    "args.mse_parameter = 0.1\n",
    "args.clean_parameter = 0.05\n",
    "args.res_parameter = 0.1\n",
    "\n",
    "#model params\n",
    "args.layers = 40\n",
    "args.widen_factor = 2\n",
    "args.droprate = 0.0\n",
    "args.ind = 5\n",
    "args.max_cycles = 2\n",
    "args.save_model = 'CNNF_superes_cifar'\n",
    "args.model_dir = 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#params cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#transforms\n",
    "train_transform_cifar = transforms.Compose(\n",
    "  [transforms.RandomHorizontalFlip(),\n",
    "   transforms.RandomCrop(32, padding=4),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize([0.5] * 3, [0.5] * 3)])\n",
    "\n",
    "test_transform_cifar = transforms.Compose(\n",
    "  [transforms.ToTensor(),\n",
    "   transforms.Normalize([0.5] * 3, [0.5] * 3)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#model and dataset\n",
    "train_data = datasets.CIFAR10(\n",
    "    'data', train=True, transform=train_transform_cifar, download=True)\n",
    "test_data = datasets.CIFAR10(\n",
    "    'data', train=False, transform=test_transform_cifar, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_data, batch_size=args.batch_size,\n",
    "  shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_data, batch_size=args.test_batch_size,\n",
    "  shuffle=True, num_workers=4, pin_memory=True)\n",
    "num_classes = 10\n",
    "model = WideResNet(args.layers, 10, args.widen_factor, args.droprate, args.ind, args.max_cycles, args.res_parameter).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(\n",
    "      model.parameters(),\n",
    "      args.lr,\n",
    "      momentum=args.momentum,\n",
    "      weight_decay=args.wd)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "  optimizer, lr_lambda=lambda step: lr_poly(1.0, step, args.epochs * len(train_loader), args.power))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m best_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(args\u001B[38;5;241m.\u001B[39mepochs)):\n\u001B[0;32m----> 5\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_adv\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcycles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_cycles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmse_parameter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_parameter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_parameter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclean_parameter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclean\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m test(args, model, device, test_loader, cycles\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mmax_cycles, epoch\u001B[38;5;241m=\u001B[39mepoch)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Github/CNNF/train.py:43\u001B[0m, in \u001B[0;36mtrain_adv\u001B[0;34m(args, model, device, train_loader, optimizer, scheduler, epoch, cycles, mse_parameter, clean_parameter, clean)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (images, targets) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m     42\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 43\u001B[0m     images \u001B[38;5;241m=\u001B[39m \u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     46\u001B[0m     model\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[0;32m~/Github/CNNF/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:221\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    224\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Begin training\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train_loss, train_acc = train_adv(args, model, device, train_loader, optimizer, scheduler, epoch,\n",
    "      cycles=args.max_cycles, mse_parameter=args.mse_parameter, clean_parameter=args.clean_parameter, clean=args.clean)\n",
    "\n",
    "    test_loss, test_acc = test(args, model, device, test_loader, cycles=args.max_cycles, epoch=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print('loss', 'train:', train_loss)\n",
    "    print('acc', 'train:', train_acc)\n",
    "    print('loss', 'test:', test_loss)\n",
    "    print('acc', 'test:', test_acc)\n",
    "\n",
    "    # Save the model with the best accuracy\n",
    "    if test_acc > best_acc and args.save_model is not None:\n",
    "        best_acc = test_acc\n",
    "        experiment_fn = args.save_model\n",
    "        torch.save(model.state_dict(),\n",
    "                   args.model_dir + \"/{}-best.pt\".format(experiment_fn))\n",
    "\n",
    "    if ((epoch+1)%50)==0 and args.save_model is not None:\n",
    "        experiment_fn = args.save_model\n",
    "        torch.save(model.state_dict(),\n",
    "                   args.model_dir + \"/{}-epoch{}.pt\".format(experiment_fn,epoch))\n",
    "        pgd_acc = test_pgd(args, model, device, test_loader, epsilon=args.eps)\n",
    "\n",
    "        print('pgd_acc', 'test:', pgd_acc)\n",
    "\n",
    "# Save final model\n",
    "if args.save_model is not None:\n",
    "    experiment_fn = args.save_model\n",
    "    torch.save(model.state_dict(),\n",
    "               args.model_dir + \"/{}.pt\".format(experiment_fn))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}